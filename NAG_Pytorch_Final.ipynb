{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ngpu=1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- Download of Dataset\n",
    "- Post Mega/Download Link for Train abd Validation data of Imagenet 2012 (Obtained from Kaggle)\n",
    "    - Validation Data: [Mega Link](https://mega.nz/#!yDoTDIyD!RjN6OBA92-KLpNqDeLS3OzwmAYesEbTsiQat9hT6p6s)\n",
    "    - Trainning Data: [Mega Link](https://mega.nz/#!vKY0WSDa!4aibnBkiXUrO9MkhQlLGXac7wLF5HY7O4LzfdFEaeQU) **P.S**: Randomly Sampled 10 instances from each target class as described in the paper.\n",
    "- If link fails to work use the following Colab notebook to generate your own subset of trainning examples. [Link](https://colab.research.google.com/drive/1LbZBfgqntWb3HuC3UFyF_FvwnHtd1xTA)\n",
    "- Setting up of Folder Structure\n",
    "For Easier handling and reproducibility of results download from mega link \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILSVRC/ ILSVRC/train ILSVRC/valid\n"
     ]
    }
   ],
   "source": [
    "dataset_path=r'ILSVRC/'\n",
    "train_dataset_path=dataset_path+'train'\n",
    "test_dataset_path=dataset_path+'valid'\n",
    "print(dataset_path,train_dataset_path,test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of Labels \n",
    "label_dict={}\n",
    "label_idx={}\n",
    "with open('ILSVRC/LOC_synset_mapping.txt') as file:\n",
    "    lines=file.readlines()\n",
    "    for idx,line in enumerate(lines):\n",
    "        label,actual =line.strip('\\n').split(' ',maxsplit=1)\n",
    "        label_dict[label]=actual\n",
    "        label_idx[label]=idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Description :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Dataloaders\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, subset, root_dir, transform=None):\n",
    "        self.root_dir=root_dir\n",
    "        self.transform=transform\n",
    "       \n",
    "        self.subset=subset\n",
    "        if self.subset=='train':\n",
    "            data_dir=os.path.join(self.root_dir,self.subset)\n",
    "            self.images_fn=glob(f'{data_dir}/*/*')\n",
    "            self.labels=[fn.split('\\\\')[1] for fn in self.images_fn]\n",
    "        elif subset =='valid':\n",
    "            df=pd.read_csv('ILSVRC/LOC_val_solution.csv')\n",
    "            df['label']=df['PredictionString'].str.split(' ',n=1,expand=True)[0]\n",
    "            df=df.drop(columns=['PredictionString'])\n",
    "            self.images_fn='ILSVRC/valid/'+df['ImageId'].values+'.JPEG'\n",
    "            self.labels=df['label']\n",
    "        else:\n",
    "            raise ValueError\n",
    "        print(f\" Number of instances in {self.subset} subset of Dataset: {len(self.images_fn)}\")       \n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        fn=self.images_fn[idx]\n",
    "        label=self.labels[idx]\n",
    "        image=Image.open(fn)\n",
    "        if image.getbands()[0] == 'L':\n",
    "            image = image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)    \n",
    "#         print(type(image))\n",
    "        return image,label_idx[label]\n",
    "\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_fn)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "size=128\n",
    "mean = [103.939, 116.779, 123.68]\n",
    "\n",
    "\n",
    "preprocess=transforms.Compose([transforms.Resize((size,size)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean,(0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of instances in train subset of Dataset: 10000\n",
      " Number of instances in valid subset of Dataset: 50000\n"
     ]
    }
   ],
   "source": [
    "data_train=CustomDataset(subset='train',root_dir=dataset_path,transform=preprocess) # Need to Fix train Data\n",
    "data_test=CustomDataset(subset='valid',root_dir=dataset_path,transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "for images,labels in tqdm(DataLoader(data_test,batch_size=100)):\n",
    "    print(images.shape,len(labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('ILSVRC/LOC_val_solution.csv')\n",
    "df['label']=df['PredictionString'].str.split(' ',n=1,expand=True)[0]\n",
    "df=df.drop(columns=['PredictionString'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Approach\n",
    "\n",
    "![Proposed approach](resources\\\\nag.png)\n",
    "\n",
    "- **Core idea is to model the distribution of universal adversarial perturbations for a given classifier.**\n",
    "- The image shows a batch of B random vectors {z}<sub>B</sub> transforming into perturbations {delta}<sub>B</sub> by G which get added to the batch of data samples {x}<sub>B</sub>.\n",
    "- The top portion shows adversarial batch (X<sub>A</sub>), bottom portion shows shuffled adversarial batch (X<sub>S</sub>) and middle portion shows the benign batch (X<sub>B</sub>). The Fooling objective Lf (eq. 2) and Diversity objective Ld (eq. 3) constitute the loss. \n",
    "### Note\n",
    "- Note that the target CNN (f) is a trained classifier and its parameters are not updated during the proposed training. On the other hand, the parameters of generator (G) are randomly initialized and learned through backpropagating the loss. (Best viewed in color)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of Hyperparameters\n",
    "- The architecture of the generator consists of 5 deconv layers. The final deconv layer is followed by a tanh non-linearity and scaling by epsillon (10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsillon=10\n",
    "# batch_size=32\n",
    "# latent_dim = 10\n",
    "# img_h,img_w,img_c=(224,224,3)\n",
    "\n",
    "latent_dim=10\n",
    "arch='googlenet'\n",
    "archs=['vgg-f','vgg16','vgg19','googlenet','resnet50','resnet152']\n",
    "if arch in ['vgg16','vgg19','vgg-f','googlenet']:\n",
    "    bs=64\n",
    "elif arch in ['resnet50','resnet152']:\n",
    "    bs=32\n",
    "else:\n",
    "    raise ValueError(f'Architecture type not supported. Please choose one from the following {archs}')\n",
    "bs\n",
    "bs=4 # OOM Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator \n",
    "- Architecture of our generator (G) unchanged for different target CNN architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DCGAN](resources/DCGAN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of ConvTranspose2d : combination of upsampling and convolution layers is equal to a strided\n",
    "# convolutional layer. increase the spatial resolution of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf=128\n",
    "nz= latent_dim\n",
    "nc=3 # Number of Channels\n",
    "\n",
    "class AdveraryGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdveraryGenerator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "        nn.ConvTranspose2d( nz, 1024, 4, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(1024),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*8) x 4 x 4\n",
    "        nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*4) x 8 x 8\n",
    "        nn.ConvTranspose2d( 512, 256, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*2) x 16 x 16\n",
    "        nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf) x 32 x 32\n",
    "        nn.ConvTranspose2d( 128, nc, 4, 4, 0, bias=False),\n",
    "        nn.Tanh()\n",
    "        # state size. (nc) x 64 x 64\n",
    "        )\n",
    "    #     def __call__(self):\n",
    "    # Dont Override the __call__ method. Pytorch does forward and backward hooks required, \n",
    "    # Always use forward method to avoid any issues\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarygen=AdveraryGenerator().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gokkulnath\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.2222e-05, 2.9776e-03, 1.0532e-04,  ..., 1.3071e-04, 1.0350e-03,\n",
       "         7.7704e-03],\n",
       "        [1.2753e-05, 1.7874e-05, 4.1048e-04,  ..., 2.0979e-04, 6.4641e-04,\n",
       "         1.8757e-03],\n",
       "        [8.6980e-05, 2.1691e-04, 1.3905e-05,  ..., 3.9865e-05, 6.9964e-04,\n",
       "         2.0398e-04],\n",
       "        [3.9080e-03, 1.6218e-04, 9.7747e-04,  ..., 6.9027e-04, 1.1081e-03,\n",
       "         9.8989e-05]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft=nn.Softmax()\n",
    "soft(clean_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1000]) torch.Size([4, 1000]) torch.Size([4, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gokkulnath\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([ 60, 126, 680, 421], device='cuda:0') tensor([406, 570, 428, 506], device='cuda:0') tensor([558, 676, 738, 577], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Train Loop \n",
    "\n",
    "# Model Architecture\n",
    "import torchvision.models as tvm\n",
    "f=tvm.resnet50(pretrained=True)\n",
    "f.to(device)#.eval()\n",
    "\n",
    "\n",
    "train_dl=DataLoader(data_train,batch_size=bs,shuffle=False)\n",
    "\n",
    "loss= nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "for i,data in enumerate(train_dl,0):\n",
    "    images=data[0].to(device)\n",
    "    labels=data[1].to(device)\n",
    "\n",
    "    noise=adversarygen(torch.randn(bs, nz, 1, 1, device=device))\n",
    "#     print(images.shape,noise.shape,labels)\n",
    "    \n",
    "    with torch.no_grad() :\n",
    "        # XB = images\n",
    "        clean_preds=f(images)\n",
    "\n",
    "        #XA = images+noise\n",
    "        preds_XA= f(images+noise)\n",
    "        '''\n",
    "        From the Paper -->\n",
    "        We also randomly shuffle the perturbations ensuring no perturbation remains in its original index in the batch\n",
    "        '''\n",
    "        #XS = images+ noise[torch.randperm(bs)] # Shuffling Logic ? Across the batch dimension or within the image ? 1. Within Image \n",
    "        preds_XS=f(images+ noise[torch.randperm(bs)])\n",
    "    \n",
    "#     print(XA.shape,XB.shape,XS.shape)\n",
    "    \n",
    "    print(preds_XA.shape,clean_preds.shape,preds_XS.shape)\n",
    "    print(i,torch.argmax(soft(preds_XA),dim=1),torch.argmax(soft(clean_preds),dim=1),torch.argmax(soft(preds_XS),dim=1))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fooling Objective\n",
    "- \n",
    "[406, 570, 428, 506]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fooling_obj= -1 * torch.log(1-clean_preds)\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do : Need to Write a Custom Model from scratch for VGG -F  and load weights from caffe model\n",
    "- Refer Here : https://github.com/val-iisc/nag/blob/83564eb4a8b5177660e2f6566dd63faa16f76773/nets/vgg_f.py\n",
    "- https://github.com/val-iisc/nag/blob/83564eb4a8b5177660e2f6566dd63faa16f76773/misc/convert_weights.py\n",
    "- Here VGGF refers to VGG-Face model  http://www.vlfeat.org/matconvnet/pretrained/. \n",
    "- How we can use that to classify Imagenet ?\n",
    "- load caffe prototxt and weights directly in pytorch --> https://github.com/marvis/pytorch-caffe\n",
    "- Convert Caffe models to Pytorch : https://github.com/vadimkantorov/caffemodel2pytorch\n",
    "#### Check this link for Conversion Tutorial : [Link](https://colab.research.google.com/drive/1i2dq6qctPvrLREhKOZNNBsNfKuaS0HYQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/nn.html?highlight=convtranspose2d#torch.nn.ConvTranspose2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Futute Scope of Work \n",
    "# Variational Auto-Encoders (VAE) --> Not Done ? Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
