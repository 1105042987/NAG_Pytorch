{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "IN_COLAB\n",
    "\n",
    "debug=False\n",
    "DOWNLOAD_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "- Download of Dataset \n",
    "**P.S**: Randomly Sampled 10 instances from each target class as described in the paper.\n",
    "- Option 1: Download from Archive.org\n",
    "    - [Archive Link](https://archive.org/details/Imagenet_NAG)\n",
    "    - [train.zip](https://archive.org/download/Imagenet_NAG/train.zip)\n",
    "    - [valid.zip](https://archive.org/download/Imagenet_NAG/valid.zip)\n",
    "- Option 2 : Mega Download Link for Train abd Validation data of Imagenet 2012 (Obtained from Kaggle)\n",
    "    - Validation Data: [Mega Link](https://mega.nz/#!yDoTDIyD!RjN6OBA92-KLpNqDeLS3OzwmAYesEbTsiQat9hT6p6s)\n",
    "    - Trainning Data: [Mega Link](https://mega.nz/#!vKY0WSDa!4aibnBkiXUrO9MkhQlLGXac7wLF5HY7O4LzfdFEaeQU) \n",
    "<!-- - If link fails to work use the following Colab notebook to generate your own subset of trainning examples. [Link](https://colab.research.google.com/drive/1LbZBfgqntWb3HuC3UFyF_FvwnHtd1xTA) -->\n",
    "- Setting up of Folder Structure\n",
    "For Easier handling and reproducibility of results download from mega link \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB or DOWNLOAD_DATA:\n",
    "    !mkdir ILSVRC\n",
    "    !sudo apt install aria2 zip -y\n",
    "    !aria2c -x 4 https://archive.org/download/Imagenet_NAG/train.zip -o train.zip\n",
    "    !aria2c -x 4 https://archive.org/download/Imagenet_NAG/valid.zip -o valid.zip\n",
    "    !wget https://archive.org/download/Imagenet_NAG/LOC_val_solution.csv -O ILSVRC/LOC_val_solution.csv\n",
    "    !wget https://archive.org/download/Imagenet_NAG/LOC_synset_mapping.txt -O ILSVRC/LOC_synset_mapping.txt\n",
    "    !unzip -qq train.zip -d ILSVRC/ \n",
    "    !unzip -qq valid.zip -d ILSVRC/\n",
    "    \n",
    "    \n",
    "    # TODO Download Data from Archive\n",
    "    # Extract and Do the Pre-Processing\n",
    "#     !rm train.zip\n",
    "#     !rm valid.zip\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Verification\n",
      "Total Number of Classes: 1000 in train directory\n",
      "Total 10000 number of files in 1000 classes. i.e 10 Images/Class\n",
      "Validation Data Verification\n",
      "Validation Data has correct number of files i.e 50000\n",
      "Dataset is Setup Correctly\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "train_ok = True\n",
    "val_ok = True\n",
    "print(\"Training Data Verification\")\n",
    "cls_count = len(glob(\"ILSVRC/train/*\"))\n",
    "print(\"Total Number of Classes: {} in train directory\".format(cls_count))\n",
    "count = 0\n",
    "for cls_ in glob(\"ILSVRC/train/*\"):\n",
    "    imgs = glob(cls_ + \"/*\")\n",
    "    img_count = len(imgs)\n",
    "    count += img_count\n",
    "    if img_count != 10:\n",
    "        print(cls_.split(\"/\")[-1], img_count)\n",
    "        train_ok=False\n",
    "print(\"Total {} number of files in {} classes. i.e 10 Images/Class\".format(count, cls_count))\n",
    "\n",
    "print(\"Validation Data Verification\")\n",
    "val_files = glob(\"ILSVRC/valid/*\")\n",
    "val_count = len(val_files)\n",
    "if val_count == 50000:\n",
    "    print(\"Validation Data has correct number of files i.e {}\".format(val_count))\n",
    "else:\n",
    "    print(\"Validation Data has some issue. Has following number of file : {}. Kindly Check!!\".format(val_count))\n",
    "    val_ok=False\n",
    "if train_ok and val_ok:\n",
    "    print(\"Dataset is Setup Correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as tvm\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.folder import DatasetFolder,ImageFolder\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os,time,gc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import datetime,random,string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pytorch Version : 1.4.0 and Torchvision Version : 0.5.0. Using Device cuda\n"
     ]
    }
   ],
   "source": [
    "ngpu=torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "print(\"Using Pytorch Version : {} and Torchvision Version : {}. Using Device {}\".format(torch.__version__,torchvision.__version__,device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloaders Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root Folder:ILSVRC/. Train Data Path: ILSVRC/train. Validation Data Path ILSVRC/valid\n"
     ]
    }
   ],
   "source": [
    "dataset_path=r'ILSVRC/'\n",
    "train_dataset_path=dataset_path+'train'\n",
    "test_dataset_path=dataset_path+'valid'\n",
    "print(\"Dataset root Folder:{}. Train Data Path: {}. Validation Data Path {}\".format(dataset_path,train_dataset_path,test_dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of Labels \n",
    "label_dict={}\n",
    "label_idx={}\n",
    "\n",
    "with open('ILSVRC/LOC_synset_mapping.txt') as file:\n",
    "    lines=file.readlines()\n",
    "    for idx,line in enumerate(lines):\n",
    "        label,actual =line.strip('\\n').split(' ',maxsplit=1)\n",
    "        label_dict[label]=actual\n",
    "        label_idx[label]=idx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "size=224\n",
    "# Imagenet Stats\n",
    "vgg_mean = [103.939, 116.779, 123.68]\n",
    "\n",
    "preprocess=transforms.Compose([transforms.Resize((size,size)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(vgg_mean,(0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of instances in valid subset of Dataset: 50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, subset, root_dir, transform=None):\n",
    "        self.root_dir=root_dir\n",
    "        self.transform=transform\n",
    "       \n",
    "        self.subset=subset\n",
    "        if self.subset=='train':\n",
    "            data_dir=os.path.join(self.root_dir,self.subset)\n",
    "            self.images_fn=glob(f'{data_dir}/*/*')\n",
    "            self.labels=[Path(fn).parent.name for fn in self.images_fn]\n",
    "        elif subset =='valid':\n",
    "            df=pd.read_csv('ILSVRC/LOC_val_solution.csv')\n",
    "            df['label']=df['PredictionString'].str.split(' ',n=1,expand=True)[0]\n",
    "            df=df.drop(columns=['PredictionString'])\n",
    "            self.images_fn='ILSVRC/valid/'+df['ImageId'].values+'.JPEG'\n",
    "            self.labels=df['label']\n",
    "        else:\n",
    "            raise ValueError\n",
    "        print(f\" Number of instances in {self.subset} subset of Dataset: {len(self.images_fn)}\")       \n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        fn=self.images_fn[idx]\n",
    "        label=self.labels[idx]\n",
    "        image=Image.open(fn)\n",
    "        if image.getbands()[0] == 'L':\n",
    "            image = image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)    \n",
    "#         print(type(image))\n",
    "        return image,label_idx[label]\n",
    "\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_fn)\n",
    "        \n",
    "data_train=ImageFolder(root='ILSVRC/train',transform=preprocess)\n",
    "class2idx=data_train.class_to_idx\n",
    "data_valid=CustomDataset(subset='valid',root_dir=dataset_path,transform=preprocess)\n",
    "\n",
    "train_num = len(data_train)\n",
    "val_num = len(data_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Proposed approach](resources/nag.png)\n",
    "\n",
    "- **Core idea is to model the distribution of universal adversarial perturbations for a given classifier.**\n",
    "- The image shows a batch of B random vectors {z}<sub>B</sub> transforming into perturbations {delta}<sub>B</sub> by G which get added to the batch of data samples {x}<sub>B</sub>.\n",
    "- The top portion shows adversarial batch (X<sub>A</sub>), bottom portion shows shuffled adversarial batch (X<sub>S</sub>) and middle portion shows the benign batch (X<sub>B</sub>). The Fooling objective Lf (eq. 2) and Diversity objective Ld (eq. 3) constitute the loss. \n",
    "### Note\n",
    "- Note that the target CNN (f) is a trained classifier and its parameters are not updated during the proposed training. On the other hand, the parameters of generator (G) are randomly initialized and learned through backpropagating the loss. (Best viewed in color)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions/Objectives \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fooling_objective(qc_):\n",
    "    '''Helper function to computer compute -log(1-qc'), \n",
    "    where qc' is the adversarial probability of the class having \n",
    "    maximum probability in the corresponding clean probability\n",
    "    qc' ---> qc_\n",
    "    Parameters: \n",
    "    prob_vec : Probability vector for the clean batch\n",
    "    adv_prob_vec : Probability vecotr of the adversarial batch\n",
    "    Returns: \n",
    "    -log(1-qc') , qc'\n",
    "    \n",
    "    '''  \n",
    "    # Get the largest probablities from predictions : Shape (bs,1)\n",
    "    qc_=qc_.mean()\n",
    "    return -1*torch.log(1-qc_) , qc_\n",
    "\n",
    "def diversity_objective(prob_vec_no_shuffle, prob_vec_shuffled):\n",
    "    '''Helper function to calculate the cosine distance between two probability vectors\n",
    "    Parameters: \n",
    "    prob_vec : Probability vector for the clean batch\n",
    "    adv_prob_vec : Probability vector for the adversarial batch\n",
    "    Returns : \n",
    "    Cosine distance between the corresponding clean and adversarial batches\n",
    "    '''    \n",
    "    return torch.cosine_similarity(prob_vec_no_shuffle,prob_vec_shuffled).mean()\n",
    "\n",
    "## TODO \n",
    "\n",
    "def intermediate_activation_objective(layer_name=None):\n",
    "    ''' Extract the activations of any intermediate layer for:\n",
    "    1. batch of images (of batch size=32) corrupted by the perturbations (of batch size=32) \n",
    "    2. same batch of images corrupted by same batch of perturbations but in different (random) order\n",
    "    (in this case the intermdeiate layer is set to 'res4f' of ResNet 50 architecture)\n",
    "    '''\n",
    "    if arch =='resnet50':\n",
    "        layer_name='res4f'\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of ConvTranspose2d : combination of upsampling and convolution layers is equal to a strided\n",
    "# convolutional layer. increase the spatial resolution of the tensor\n",
    "#     def __call__(self):\n",
    "# Dont Override the __call__ method. Pytorch does forward and backward hooks required, \n",
    "# Always use forward method to avoid any issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator \n",
    "- Architecture of our generator (G) unchanged for different target CNN architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DCGAN](resources/DCGAN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "ngf=128\n",
    "nz= latent_dim=10\n",
    "e_lim = 10\n",
    "nc=3 # Number of Channels\n",
    "\n",
    "# Fixed Architecture: Weights will be updated by Backprop.\n",
    "class AdveraryGenerator(nn.Module):\n",
    "    def __init__(self,e_lim):\n",
    "        super(AdveraryGenerator, self).__init__()\n",
    "        self.e_lim = e_lim\n",
    "        self.main = nn.Sequential(\n",
    "        nn.ConvTranspose2d( in_channels=nz,out_channels= 1024, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(1024),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*8) x 4 x 4\n",
    "        nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*4) x 8 x 8\n",
    "        nn.ConvTranspose2d( 512, 256, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*2) x 16 x 16\n",
    "        nn.ConvTranspose2d(256, 128, 4, 2, 2, bias=False),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf) x 32 x 32\n",
    "        nn.ConvTranspose2d( 128, 64, 4, 2, 2, bias=False),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (nc) x 64 x 64\n",
    "        nn.ConvTranspose2d( 64, 3, 4, 4,4, bias=False),\n",
    "        nn.BatchNorm2d(3),\n",
    "        nn.ReLU(True),\n",
    "        nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.e_lim * self.main(x) # Scaling of ε\n",
    "    \n",
    "    \n",
    "adversarygen=AdveraryGenerator(e_lim).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    try:\n",
    "        from torchsummary import summary\n",
    "        summary(adversarygen,(nz,1,1))\n",
    "    except:\n",
    "        raise('Check torchsummary is installed. If not install using the command pip install torchsummary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Discriminator : Model : Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import googlenet, vgg16 , vgg19, resnet152, resnet50\n",
    "\n",
    "\n",
    "model_dict ={\n",
    "    'googlenet': googlenet,\n",
    "    'vgg16': vgg16 ,\n",
    "    'vgg19':vgg19, \n",
    "    'resnet152':resnet152, \n",
    "    'resnet50':resnet50 \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of Hyperparameters\n",
    "- The architecture of the generator consists of 5 deconv layers. The final deconv layer is followed by a tanh non-linearity and scaling by epsillon (10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Pretrained Weights:\n",
    "# for arch in model_dict.keys():\n",
    "#     model=model_dict[arch](pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epsillon=10\n",
    "# batch_size=32\n",
    "# latent_dim = 10\n",
    "img_h,img_w,img_c=(224,224,3)\n",
    "latent_dim=10\n",
    "arch='resnet50'\n",
    "archs=model_dict.keys() # ['vgg-f','vgg16','vgg19','googlenet','resnet50','resnet152'] \n",
    "\n",
    "def get_bs(arch):\n",
    "    if torch.cuda.is_available():\n",
    "#         GPU_BENCHMARK= 8192.0\n",
    "#         GPU_MAX_MEM = torch.cuda.get_device_properties(device).total_memory / (1024*1024)\n",
    "#         BS_DIV= GPU_BENCHMARK/GPU_MAX_MEM\n",
    "#         print(f\"Current GPU MAX Size : {GPU_MAX_MEM}. {BS_DIV}\")\n",
    "\n",
    "        if arch  not in ['resnet50','resnet152']:#  ['vgg16','vgg19','vgg-f','googlenet']:\n",
    "            bs=int(64)\n",
    "        elif arch in ['resnet50','resnet152']:\n",
    "            bs=int(32)\n",
    "        else:\n",
    "            raise ValueError(f'Architecture type not supported. Please choose one from the following {archs}')\n",
    "    else:\n",
    "        bs=8 # OOM Error\n",
    "    return bs\n",
    "\n",
    "get_bs(arch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=model_dict[arch](pretrained=True)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, to_save, filename='checkpoint.pth'):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if to_save:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(model.state_dict(), filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")\n",
    "        \n",
    "def save_perturbations(noise,arch,epoch,wabdb_flag=False):\n",
    "    rand_str= ''.join( random.choice(string.ascii_letters) for i in range(6))\n",
    "    os.makedirs(f\"{arch}-{rand_str}\",exist_ok=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    perturbations=noise.permute(0,2,3,1).cpu().detach().numpy()*255\n",
    "    np.save(f'{arch}-{rand_str}/Perturbations_{arch}_{epoch}.npy', perturbations)\n",
    "    for perturb_idx,perturbation in enumerate(perturbations[:,]):\n",
    "        \n",
    "        im = Image.fromarray(perturbation.astype(np.uint8))\n",
    "        if wabdb_flag:\n",
    "            wandb.log({\"noise\": [wandb.Image(im, caption=f\"Noise_{arch}_{epoch}_{perturb_idx}\")]})\n",
    "        im.save(f'{arch}-{rand_str}/Perturbations_{arch}_{epoch}_{perturb_idx}.png')        \n",
    "\n",
    "# TODO \n",
    "def visualize_perturbations():\n",
    "    # MAtplotlib Subplot ?\n",
    "    # Subplots(4*4) or (3*3)\n",
    "    # From Memory or Disk - Epoch number ?\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def get_preds(predictions,return_idx=False, k=1):\n",
    "    idxs= torch.argsort(predictions,descending=True)[:,:k]\n",
    "    if return_idx:\n",
    "        return predictions[:,idxs], idxs\n",
    "    return  predictions[:,idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_iterations = val_num/bs\n",
    "\n",
    "def compute_fooling_rate(prob_adv,prob_real):\n",
    "    '''Helper function to calculate mismatches in the top index vector\n",
    "     for clean and adversarial batch\n",
    "     Parameters:\n",
    "     prob_adv : Index vector for adversarial batch\n",
    "     prob_real : Index vector for clean batch\n",
    "     Returns:\n",
    "     Number of mismatch and its percentage\n",
    "    '''\n",
    "    nfool=0\n",
    "    size = prob_real.shape[0]\n",
    "    for i in range(size):\n",
    "        if prob_real[i]!=prob_adv[i]:\n",
    "            nfool = nfool+1\n",
    "    return nfool, 100*float(nfool)/size      \n",
    "\n",
    "\n",
    "def validate_generator_old(noise,val_dl,val_iterations=10):\n",
    "    \n",
    "    total_fool=0\n",
    "    print(\"############### VALIDATION PHASE STARTED ################\")\n",
    "    train_log.writelines(\"############### VALIDATION PHASE STARTED ################\")\n",
    "    \n",
    "    for val_idx in range(val_iterations):\n",
    "        for batch_idx, data in enumerate(val_dl):\n",
    "            images = data[0].to(device)\n",
    "#             labels = data[1].to(device)\n",
    "            \n",
    "            prob_vec_clean = F.softmax(D_model(images),dim=0) # Variable q\n",
    "            prob_vec_no_shuffle = D_model(images + noise)  \n",
    "            nfool, _ = compute_fooling_rate(prob_vec_no_shuffle,prob_vec_clean)\n",
    "            total_fool += nfool\n",
    "    \n",
    "    \n",
    "    fool_rate = 100*float(total_fool)/(val_iterations*batch_size)       \n",
    "    print(f\"Fooling rate: {foolr}. Total Items Fooled :{total_fool}\")\n",
    "    train_log.writelines(f\"Fooling rate: {foolr}. Total Items Fooled :{total_fool}\")\n",
    "\n",
    "    \n",
    "    \n",
    "def validate_generator(noise,D_model,val_dl):\n",
    "    total_fool=0\n",
    "    for batch_idx, data in tqdm(enumerate(val_dl),total = val_num//val_dl.batch_size):\n",
    "        val_images = data[0].to(device)\n",
    "        val_labels = data[1].to(device)\n",
    "\n",
    "        prob_vec_clean,clean_idx = get_preds(F.softmax(D_model(val_images),dim=0),return_idx=True) # Variable q\n",
    "        prob_vec_no_shuffle,adv_idx = get_preds(F.softmax(D_model(val_images + noise),dim=0),return_idx=True)  \n",
    "        nfool, _ = compute_fooling_rate(adv_idx,clean_idx)\n",
    "        total_fool += nfool\n",
    "\n",
    "    fool_rate = 100*float(total_fool)/(val_num)\n",
    "    return fool_rate,total_fool\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test  Fooling Objective\n",
    "adv = torch.randint(0,1000,(32,1))\n",
    "real = torch.randint(0,1000,(32,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/gokkulnath/NAG_Pytorch\" target=\"_blank\">https://app.wandb.ai/gokkulnath/NAG_Pytorch</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/gokkulnath/NAG_Pytorch/runs/2jq8reav\" target=\"_blank\">https://app.wandb.ai/gokkulnath/NAG_Pytorch/runs/2jq8reav</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/gokkulnath/NAG_Pytorch/runs/2jq8reav"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup Wandb \n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"NAG_Pytorch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Train the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'b', 'E', 'A', 'F', 's']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ random.choice(string.ascii_letters) for i in range(6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit(nb_epochs,D_model,dls,optimizer,adversarygen=adversarygen):\n",
    "    # Set the Discriminator in Eval mode; Weights are fixed.\n",
    "    train_dl,val_dl = dls\n",
    "    D_model=D_model.to(device)\n",
    "    D_model.eval()\n",
    "    timestamp=datetime.datetime.now().strftime(\"%d%b%Y_%H_%M\")\n",
    "    train_log = open(f'train_log_{arch}_{timestamp}.txt','w')\n",
    "    for epoch in tqdm(range(nb_epochs),total=nb_epochs):\n",
    "        running_loss=0\n",
    "        rand_str= ''.join( random.choice(string.ascii_letters) for i in range(6))\n",
    "        \n",
    "        train_log.writelines(f\"############### TRAIN PHASE STARTED : {epoch}################\")\n",
    "        for batch_idx, data in tqdm(enumerate(train_dl),total = train_num//train_dl.batch_size):\n",
    "            # Move Data and Labels to device(GPU)\n",
    "            images = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            \n",
    "            # Generate the Adversarial Noise from Uniform Distribution U[-1,1]\n",
    "            latent_seed = 2 * torch.rand(bs, nz, 1, 1, device=device,requires_grad=True) -1 # (r1 - r2) * torch.rand(a, b) + r2\n",
    "            noise = adversarygen(latent_seed)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # XB = images\n",
    "            #preds_XB = f(images)\n",
    "            prob_vec_clean = F.softmax(D_model(images),dim=0) # Variable q\n",
    "            clean_preds ,clean_idx = get_preds(prob_vec_clean,return_idx=True,k=1)\n",
    "            \n",
    "            #XA = images+noise\n",
    "            #preds_XA = f(images + noise)\n",
    "            prob_vec_no_shuffle = D_model(images + noise)  \n",
    "            qc_ =  F.softmax(prob_vec_no_shuffle,dim=0).gather(1,clean_idx) # Variable q'c\n",
    "\n",
    "            # 1. fooling_objective: encourages G to generate perturbations that decrease confidence of benign predictions\n",
    "            fool_obj, mean_qc_ = fooling_objective(qc_)\n",
    "            # Perturbations  are shuffled across the batch dimesion to improve diversity\n",
    "            #XS = images+ noise[torch.randperm(bs)]\n",
    "            prob_vec_shuffled =   D_model(images + noise[torch.randperm(bs)])\n",
    "            \n",
    "            # 2.  encourages Generator to explore the space of perturbations and generate a diverse set of perturbations\n",
    "            divesity_obj=diversity_objective(prob_vec_no_shuffle, prob_vec_shuffled)\n",
    "\n",
    "            # Compute Total Loss\n",
    "            total_loss = divesity_obj + fool_obj\n",
    "            \n",
    "            # Lets perform Backpropagation to compute Gradients and update the weights\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # wandb Logging \n",
    "#             perturbations=noise.permute(0,2,3,1).cpu().detach().numpy()*255\n",
    "#             for perturb_idx,perturbation in enumerate(perturbations[:,]):\n",
    "#                 im = Image.fromarray(perturbation.astype(np.uint8))\n",
    "#                 wandb.log({\"noise\": [wandb.Image(im, caption=f\"Noise_{arch}_{epoch}_{perturb_idx}\")]})\n",
    "            wandb.log({\"fool_obj\": fool_obj.item(),\n",
    "                       \"divesity_obj\": divesity_obj.item(),\n",
    "                       \"total_loss\":total_loss.item(),\n",
    "                      })        \n",
    "            \n",
    "            running_loss += total_loss.item()\n",
    "            \n",
    "            if batch_idx!=0  and batch_idx % 100 ==0 :\n",
    "                train_log.writelines(f\"############### VALIDATION PHASE STARTED : {epoch}, Step : {int(batch_idx / 100)} ################\")\n",
    "                fool_rate,total_fool= validate_generator(noise,D_model,val_dl)\n",
    "                print(f\"Fooling rate: {fool_rate}. Total Items Fooled :{total_fool}\")\n",
    "                train_log.writelines(f\"Fooling rate: {fool_rate}. Total Items Fooled :{total_fool}\")\n",
    "        print(f\"Diversity Loss :{divesity_obj.item()} \\n Fooling Loss: {fool_obj.item()} \\n\")\n",
    "        print(f\"Total Loss after Epoch No: {epoch +1} - {running_loss/(train_num//train_dl.batch_size)}\")\n",
    "        train_log.writelines(f\"Loss after Epoch No: {epoch +1} is {running_loss/(train_num//train_dl.batch_size)}\")\n",
    "        # to_save can be any expression/condition that returns a bool\n",
    "        \n",
    "        save_checkpoint(adversarygen, to_save= True, filename=f'GeneratorW_{arch}_{epoch}_{rand_str}.pth') \n",
    "        if epoch % 1 == 0:\n",
    "#             save_perturbations(noise,arch,epoch)\n",
    "            save_perturbations(noise,arch,epoch,wabdb_flag=True)\n",
    "    train_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Actual Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Generator for Arch googlenet\n",
      "64\n",
      "Elsasped Time 0.18381357192993164 Seconds\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 20\n",
    "lr = 1e-3\n",
    "\n",
    "# Setting up Dataloaders\n",
    "import time,gc\n",
    "\n",
    "# for arch in model_dict.keys():\n",
    "arch='googlenet'\n",
    "start= time.time()\n",
    "print(f\"Training Generator for Arch {arch}\")\n",
    "model= model_dict[arch](pretrained=True)\n",
    "bs = get_bs(arch)\n",
    "print(bs)\n",
    "train_dl=DataLoader(data_train,batch_size=bs,shuffle=True,num_workers=4,pin_memory=True,drop_last=True)\n",
    "val_dl=DataLoader(data_valid,batch_size=bs,shuffle=True,num_workers=4,pin_memory=True,drop_last=True)\n",
    "dls = [train_dl,val_dl]\n",
    "optimizer = optim.Adam(adversarygen.parameters(), lr=lr)\n",
    "\n",
    "# del model, train_dl, val_dl,dls ,optimizer\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "print(f\"Elsasped Time {time.time()-start} Seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcbd1d48a3f4fe081c6c7facc57bf34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea83dbbb73a4bd0b677337329a5a642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:45: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871a8e93084a478d9792c43a8fb1e06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.664. Total Items Fooled :48332\n",
      "\n",
      "Diversity Loss :0.9685168266296387 \n",
      " Fooling Loss: 0.018528591841459274 \n",
      "\n",
      "Total Loss after Epoch No: 1 - 1.0073822904855778\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e83fb2ace44a10abdf8d58fea48994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdea897a7fa4f30aee8237b96ed0094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 97.31. Total Items Fooled :48655\n",
      "\n",
      "Diversity Loss :0.8594930768013 \n",
      " Fooling Loss: 0.020314300432801247 \n",
      "\n",
      "Total Loss after Epoch No: 2 - 0.9293945748836566\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb9f1fe57cb4131a51cab1b5ca5b47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4ff501f336492087b6bebc50135ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.518. Total Items Fooled :48259\n",
      "\n",
      "Diversity Loss :0.83614182472229 \n",
      " Fooling Loss: 0.01818085089325905 \n",
      "\n",
      "Total Loss after Epoch No: 3 - 0.8600391764671375\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4978b2d3a249beaaeab7034d51cf9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d538c9c40dc47bdbcecb2b41cab9d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.312. Total Items Fooled :48156\n",
      "\n",
      "Diversity Loss :0.7696624994277954 \n",
      " Fooling Loss: 0.013310590758919716 \n",
      "\n",
      "Total Loss after Epoch No: 4 - 0.832364702071899\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255d346d8a9f42ab9f13501b9ef4f5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e0151a06f7402b955583b6be788689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.882. Total Items Fooled :48441\n",
      "\n",
      "Diversity Loss :0.8135642409324646 \n",
      " Fooling Loss: 0.016396520659327507 \n",
      "\n",
      "Total Loss after Epoch No: 5 - 0.8169867728765194\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a2360da1e445a5a928867543730ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08536b3613b843df9664c94d1e153340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.564. Total Items Fooled :48282\n",
      "\n",
      "Diversity Loss :0.7769788503646851 \n",
      " Fooling Loss: 0.012949262745678425 \n",
      "\n",
      "Total Loss after Epoch No: 6 - 0.8055060616670511\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020cdc9bd96245c89a9d5ec7f90377fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4509951b7f4b3b840cee20cae2f35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.516. Total Items Fooled :48258\n",
      "\n",
      "Diversity Loss :0.7386317253112793 \n",
      " Fooling Loss: 0.017167219892144203 \n",
      "\n",
      "Total Loss after Epoch No: 7 - 0.7983434135333086\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f398c7f4d1114dc3b1895575bb79e5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2853c6b066c462cae2c9478db635649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.642. Total Items Fooled :48321\n",
      "\n",
      "Diversity Loss :0.7766595482826233 \n",
      " Fooling Loss: 0.017424535006284714 \n",
      "\n",
      "Total Loss after Epoch No: 8 - 0.790563125640918\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fad2cf070cb46ada470ff25e423e9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535f574f962d4ffc9a680d13168922f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.012. Total Items Fooled :48006\n",
      "\n",
      "Diversity Loss :0.7960213422775269 \n",
      " Fooling Loss: 0.01762743666768074 \n",
      "\n",
      "Total Loss after Epoch No: 9 - 0.7913590551186831\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872d034983c04271b0b7bbc12308eda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2761362721f241c3829b55b30736d74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 97.25. Total Items Fooled :48625\n",
      "\n",
      "Diversity Loss :0.7912931442260742 \n",
      " Fooling Loss: 0.014602424576878548 \n",
      "\n",
      "Total Loss after Epoch No: 10 - 0.7817597966163586\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6653df96fea14564bb8995bab71c5ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ce4c1c17de44d3a01dda7b9b4d4483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.67. Total Items Fooled :48335\n",
      "\n",
      "Diversity Loss :0.7930148839950562 \n",
      " Fooling Loss: 0.011067915707826614 \n",
      "\n",
      "Total Loss after Epoch No: 11 - 0.7848356335591047\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6896d73047846e48326f1e4292ad1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc946877d0d4a989ddf337704c907d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.92. Total Items Fooled :48460\n",
      "\n",
      "Diversity Loss :0.7839921712875366 \n",
      " Fooling Loss: 0.017957143485546112 \n",
      "\n",
      "Total Loss after Epoch No: 12 - 0.7759162856218143\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a30851f6c2479080057dc977083219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e9cd46819843b9a4efefa9ae53b968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.59. Total Items Fooled :48295\n",
      "\n",
      "Diversity Loss :0.7277077436447144 \n",
      " Fooling Loss: 0.013161827810108662 \n",
      "\n",
      "Total Loss after Epoch No: 13 - 0.7744024113202707\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb51b84567846de8644a4fb3a628ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbcc7856a294cd48854d2b6f8cbdc18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.394. Total Items Fooled :48197\n",
      "\n",
      "Diversity Loss :0.7482023239135742 \n",
      " Fooling Loss: 0.015351101756095886 \n",
      "\n",
      "Total Loss after Epoch No: 14 - 0.7730381775360841\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b7b5dc8319408f878c1d11f7737330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fb1e2c673e4b3584205cba0a06b072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.268. Total Items Fooled :48134\n",
      "\n",
      "Diversity Loss :0.6918954849243164 \n",
      " Fooling Loss: 0.014773480594158173 \n",
      "\n",
      "Total Loss after Epoch No: 15 - 0.7724726249774297\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d893b57891614290b4d79c9285ea63bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0903915ab74a7bba3828e590ace058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.838. Total Items Fooled :48419\n",
      "\n",
      "Diversity Loss :0.7407758235931396 \n",
      " Fooling Loss: 0.02140353061258793 \n",
      "\n",
      "Total Loss after Epoch No: 16 - 0.7643355555259265\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935c224149234436b94782d88547e053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2b203d0aaf40bab045e7641e9c8daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.792. Total Items Fooled :48396\n",
      "\n",
      "Diversity Loss :0.7163825035095215 \n",
      " Fooling Loss: 0.01921805366873741 \n",
      "\n",
      "Total Loss after Epoch No: 17 - 0.7648925922619991\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32590e89f0a24d5d875c1eb182ae1309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06891d56953c4c9bbdb3908cd2089839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.502. Total Items Fooled :48251\n",
      "\n",
      "Diversity Loss :0.7594460248947144 \n",
      " Fooling Loss: 0.020938649773597717 \n",
      "\n",
      "Total Loss after Epoch No: 18 - 0.760070740030362\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f18dfeef838447a9fb28504b3300c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214dee523ade4a009d29e1fe52f7641d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.504. Total Items Fooled :48252\n",
      "\n",
      "Diversity Loss :0.7315672636032104 \n",
      " Fooling Loss: 0.015688534826040268 \n",
      "\n",
      "Total Loss after Epoch No: 19 - 0.75711161394914\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc92ce8e5f34fcd8ac1fe6d5a1abb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa969ba0daee4fab9d349712d184d456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fooling rate: 96.138. Total Items Fooled :48069\n",
      "\n",
      "Diversity Loss :0.7479978799819946 \n",
      " Fooling Loss: 0.01563749462366104 \n",
      "\n",
      "Total Loss after Epoch No: 20 - 0.7516315950032992\n",
      "=> Saving a new best\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(nb_epochs=total_epochs,D_model=model,dls=dls,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  with tqdm(total = len(traindata)) as epoch_pbar:\n",
    "#                 epoch_pbar.set_description(f'Epoch {epoch}')\n",
    "# https://discuss.pytorch.org/t/training-a-model-via-a-train-method/58567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup Caffenet and VGG-F  (TODO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading VGG-F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do : Need to Write a Custom Model from scratch for VGG -F  and load weights from caffe model\n",
    "- Refer Here : https://github.com/val-iisc/nag/blob/83564eb4a8b5177660e2f6566dd63faa16f76773/nets/vgg_f.py\n",
    "- https://github.com/val-iisc/nag/blob/83564eb4a8b5177660e2f6566dd63faa16f76773/misc/convert_weights.py\n",
    "- Here VGGF refers to VGG-Face model  http://www.vlfeat.org/matconvnet/pretrained/. \n",
    "- How we can use that to classify Imagenet ?\n",
    "- load caffe prototxt and weights directly in pytorch --> https://github.com/marvis/pytorch-caffe\n",
    "- Convert Caffe models to Pytorch : https://github.com/vadimkantorov/caffemodel2pytorch\n",
    "#### Check this link for Conversion Tutorial : [Link](https://colab.research.google.com/drive/1i2dq6qctPvrLREhKOZNNBsNfKuaS0HYQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Trained Weights from Google Drive  (TODO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO\n",
    "# !pip install gdown\n",
    "# import gdown\n",
    "# import zipfile\n",
    "\n",
    "# url = 'https://drive.google.com/uc?id=0B9P1L--7Wd2vU3VUVlFnbTgtS2c' # Need to update actual URL\n",
    "# output = 'weights.zip'\n",
    "# gdown.download(url, output, quiet=True)\n",
    "\n",
    "# zipdata = zipfile.ZipFile('weights.zip')\n",
    "# zipinfos = zipdata.infolist()\n",
    "# for zipinfo in zipinfos:\n",
    "#     zipdata.extract(zipinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating NAG performance across Models: For Tabular Column Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question to be Answered \n",
    "- In the Repo code train_generator.py \n",
    "L241 : feature_loss = -10*tf.reduce_mean(tf.squared_difference(f1_res4f,f2_res4f))#feature_distance(f1_res4f,f2_res4f)\n",
    "# Why is it computed and multiplied by 10 ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to evaluate the perturbations generated by Generator Network (TODO)\n",
    "arch='Fixed'\n",
    "for modelarch, model in model_dict.items():\n",
    "    num_iteration = 10 # Blackbox Settings\n",
    "    if modelarch == arch:\n",
    "        num_iteration =100 # Whitebox Settings \n",
    "    for i range(num_iteration)\n",
    "        1. Load the Weights of the Generator\n",
    "        2. Generate a Perturbation using a random vector of dimension latent_dim,1\n",
    "        3. Add the noise to a sample image \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- GAN Architecture : Pytorch Tutorial\n",
    "- [Transpose Convolution Docs](https://pytorch.org/docs/stable/nn.html?highlight=convtranspose2d#torch.nn.ConvTranspose2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def fit(nb_epochs,D_model,dls,optimizer,adversarygen=adversarygen):\n",
    "#     # Set the Discriminator in Eval mode; Weights are fixed.\n",
    "#     train_dl,val_dl = dls\n",
    "#     D_model=D_model.to(device)\n",
    "#     D_model.eval()\n",
    "#     train_log = open(f'train_log_{arch}.txt','w')\n",
    "#     for epoch in tqdm(range(nb_epochs),total=nb_epochs):\n",
    "#         running_loss=0\n",
    "#         print(f\"############### TRAIN PHASE STARTED : {epoch} ################\")\n",
    "#         train_log.writelines(f\"############### TRAIN PHASE STARTED : {epoch}################\")\n",
    "#         tic=time.time()\n",
    "#         for batch_idx, data in tqdm(enumerate(train_dl),total = train_num//train_dl.batch_size):\n",
    "#             # Move Data and Labels to device(GPU)\n",
    "#             images = data[0].to(device)\n",
    "#             labels = data[1].to(device)\n",
    "            \n",
    "            \n",
    "#             # Generate the Adversarial Noise from Uniform Distribution U[-1,1]\n",
    "#             latent_seed = 2 * torch.rand(bs, nz, 1, 1, device=device,requires_grad=True) -1 # (r1 - r2) * torch.rand(a, b) + r2\n",
    "#             noise = adversarygen(latent_seed)\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # XB = images\n",
    "#             #preds_XB = f(images)\n",
    "#             prob_vec_clean = F.softmax(D_model(images),dim=0) # Variable q\n",
    "#             _ ,clean_idx = get_preds(prob_vec_clean,return_idx=True,k=1)\n",
    "            \n",
    "#             #XA = images+noise\n",
    "#             #preds_XA = f(images + noise)\n",
    "#             prob_vec_no_shuffle = D_model(images + noise)  \n",
    "#             qc_ =  F.softmax(prob_vec_no_shuffle,dim=0)[:,clean_idx] # Variable q'c\n",
    "\n",
    "#             # 1. fooling_objective: encourages G to generate perturbations that decrease confidence of benign predictions\n",
    "\n",
    "#             fool_obj, mean_qc_ = fooling_objective(qc_)\n",
    "\n",
    "#             # Perturbations  are shuffled across the batch dimesion to improve diversity\n",
    "#             #XS = images+ noise[torch.randperm(bs)]\n",
    "#             prob_vec_shuffled =   D_model(images + noise[torch.randperm(bs)])\n",
    "            \n",
    "#             # 2.  encourages Generator to explore the space of perturbations and generate a diverse set of perturbations\n",
    "#             divesity_obj=diversity_objective(prob_vec_no_shuffle, prob_vec_shuffled)\n",
    "\n",
    "#             # Compute Total Loss\n",
    "#             total_loss = divesity_obj+fool_obj\n",
    "\n",
    "#             # Lets perform Backpropagation to compute Gradients and update the weights\n",
    "#             total_loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             running_loss += total_loss.item()\n",
    "            \n",
    "#             if batch_idx!=0 and batch_idx % 100 ==0 :\n",
    "#                 print(f\"############### VALIDATION PHASE STARTED : {epoch}, Step :{int(batch_idx / 100)} ################\")\n",
    "#                 train_log.writelines(f\"############### VALIDATION PHASE STARTED : {epoch}, Step :{int(batch_idx / 100)} ################\")\n",
    "#                 ticval=time.time()\n",
    "#                 fool_rate,total_fool= validate_generator(noise,D_model,val_dl)\n",
    "#                 print(f\"Fooling rate: {fool_rate}. Total Items Fooled :{total_fool}\")\n",
    "#                 train_log.writelines(f\"Fooling rate: {fool_rate}. Total Items Fooled :{total_fool}\")\n",
    "#                 print(f\"Time Elasped for Validating: {time.time()-ticval} Seconds\")\n",
    "\n",
    "#         print(f\"Loss after Epoch No: {epoch +1} is {running_loss}\")\n",
    "#         train_log.writelines(f\"Loss after Epoch No: {epoch +1} is {running_loss}\")\n",
    "#         # to_save can be any expression/condition that returns a bool\n",
    "#         save_checkpoint(adversarygen, to_save= True, filename=f'GeneratorW_{arch}_{epoch}.pth') \n",
    "#         if epoch % 1 == 0:\n",
    "#             save_perturbations(noise,arch,epoch)\n",
    "#         print(f\"Time Elasped for Trainning one Epoch : {time.time()-tic} Seconds\")\n",
    "#     train_log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
